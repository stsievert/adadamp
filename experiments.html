

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Experiments &mdash; adadamp  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/basic.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API" href="api.html" />
    <link rel="prev" title="Mathematical underpinnings" href="math.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> adadamp
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic-usage.html">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Mathematical underpinnings</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Experiments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#synthetic-experiments">Synthetic experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-system-setup">Distributed system setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-experiments">Distributed experiments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">adadamp</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Experiments</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/experiments.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">

           <div itemprop="articleBody">
            
  <div class="section" id="experiments">
<span id="sec-exps"></span><h1>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h1>
<p>In <a class="reference internal" href="math.html#sec-math"><span class="std std-ref">Mathematical underpinnings</span></a>, we showed the method we proposed should require far fewer
<strong>model updates</strong> than SGD while requiring no more floating point operations.
The number of model updates can be a good proxy for time spent training, at
least with a particular setup of a distributed system <a class="reference internal" href="#imagenet-1hr" id="id1"><span>[imagenet-1hr]</span></a>.</p>
<p>This page will show experimental results that show using the proposed batch
size method results in spending less time training ML models. This will require
a particular configuration of a distributed system.</p>
<div class="section" id="synthetic-experiments">
<h2>Synthetic experiments<a class="headerlink" href="#synthetic-experiments" title="Permalink to this headline">¶</a></h2>
<p><em>To be updated shortly with experimental results that confirm</em> <a class="reference internal" href="math.html#sec-math"><span class="std std-ref">Mathematical underpinnings</span></a>.
<em>These experiments will be improved versions of the experiments in Section 6.1
of</em> <a class="footnote-reference brackets" href="#id5" id="id2">1</a>.</p>
<p>First, let’s illustrate that the proposed batch size method requires far fewer
model updates. To do this, let’s train a neural network (without any activation
function) on synthetic data.</p>
<div class="math notranslate nohighlight">
\[\min_{w_3, W_2, W_1} \sum_{i=1}^{n} (y_i - w_3^T W_2 W_1)^2\]</div>
<p>This is a remarkably complicated way to perform linear regression model, but it
does introduce some more complexity into the loss function (mostly
non-convexity). When we run our adaptive method against other competing
methods, we get these results:</p>
<div class="figure align-center" id="id7">
<img alt="_images/synth.svg" src="_images/synth.svg" /><p class="caption"><span class="caption-text">The loss for various optimizers on the neural network above. Lower values
imply a better model. The test loss of the ERM minimizer is shown with the
dashed line.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>As expected, we find that <a class="reference internal" href="api.html#adadamp.AdaDamp" title="adadamp.AdaDamp"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaDamp</span></code></a> has a minimal number of
model updates and minimal computation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="api.html#adadamp.AdaDamp" title="adadamp.AdaDamp"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaDamp</span></code></a> has the number of model updates as gradient descent</p></li>
<li><p><a class="reference internal" href="api.html#adadamp.AdaDamp" title="adadamp.AdaDamp"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaDamp</span></code></a> does no more computation than
<code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></p></li>
</ul>
<p>Of course, the batch size of AdaDamp is intense to compute. However, there are
some workarounds to this; the batch size grows closely follows an exponential
function as if the bounds were tight. This is some of the motivation behind
<a class="reference internal" href="api.html#adadamp.PadaDamp" title="adadamp.PadaDamp"><code class="xref py py-class docutils literal notranslate"><span class="pre">PadaDamp</span></code></a>.</p>
</div>
<div class="section" id="distributed-system-setup">
<h2>Distributed system setup<a class="headerlink" href="#distributed-system-setup" title="Permalink to this headline">¶</a></h2>
<p>Training ML models more quickly requires growing the number of workers with the
batch size. That is, if the batch size is 256 there should be 8 workers so each
worker computes gradients for 32 examples. If the batch size if 512, there
should be 16 workers so the number of gradients computed per worker is
constant.</p>
<p>When the distributed is configured this way, a model update takes the same
amount time regardless of the batch size <a class="reference internal" href="#imagenet-1hr" id="id3"><span>[imagenet-1hr]</span></a>. In their experiments,
they increase the batch size by <span class="math notranslate nohighlight">\(44\times\)</span> and the model update time
only takes <span class="math notranslate nohighlight">\(1.12\times\)</span> longer. That’s shown in their Figure 7:</p>
<a class="reference internal image-reference" href="_images/goyal-fig7-edits.png"><img alt="_images/goyal-fig7-edits.png" class="align-center" src="_images/goyal-fig7-edits.png" style="width: 60%;" /></a>
</div>
<div class="section" id="distributed-experiments">
<h2>Distributed experiments<a class="headerlink" href="#distributed-experiments" title="Permalink to this headline">¶</a></h2>
<p><em>Some experiments are (briefly) described below. I am in the process of
finishing the simulations on 2021-03-16.</em></p>
<p>We have mirrored the experiments in Section 5.1 of Smith et al. <a class="footnote-reference brackets" href="#id6" id="id4">2</a>. In these
experiments, they present a particular schedule of increasing the batch size
that reduces the number of model updates. We have taken their model (a
Wide-ResNet model) and dataset (CIFAR10), and followed the same batch size
schedule. They run 5 optimizers that only reduce the learning rate or increase
the batch size. All other hyperparameters for the optimizer (momentum, weight
decay, etc) are the same for every optimizer.</p>
<p>Our software has reproduced their results:</p>
<a class="reference internal image-reference" href="_images/reproduce.png"><img alt="_images/reproduce.png" class="align-center" src="_images/reproduce.png" style="width: 100%;" /></a>
<p>The optimizer labeled “Dec. LR” only decreases the learning rate, and the
optimizer labeled “Inc. BS” only increases the batch size. The optimziers
labeled “Hybrid (inc-<span class="math notranslate nohighlight">\(n\)</span>)” increase the batch size <span class="math notranslate nohighlight">\(n\)</span> times then
start decaying the learning rate afterwards. The experiment labeled “Hybrid*2”
has an initial batch size of 256 (instead of 128).</p>
<p>This model has been trained through our software, AdaDamp on NVIDIA T4 GPUs (on
an Amazon EC2 <code class="docutils literal notranslate"><span class="pre">g4dn.xlarge</span></code> instance). When we grow the number of Dask
workers with the batch size, these results are produced:</p>
<a class="reference internal image-reference" href="_images/training_time.png"><img alt="_images/training_time.png" class="align-center" src="_images/training_time.png" style="width: 50%;" /></a>
<p>This is a simulation. It sets the training time to be the training time on the
NVIDIA T4 GPU, and has (very) moderate network bandwidth. We plan to run more
simulations that improve the network bandwidth.</p>
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>“Improving the convergence of SGD with adaptive batch
sizes”.  Scott Sievert and Zachary Charles. 2019.
<a class="reference external" href="https://arxiv.org/abs/1910.08222">https://arxiv.org/abs/1910.08222</a></p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>“Don’t Decay the Learning Rate, Increase the Batch Size”. Samuel L.
Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. 2017.
<a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a></p>
</dd>
</dl>
<dl class="citation">
<dt class="label" id="imagenet-1hr"><span class="brackets">imagenet-1hr</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Section 5.5 of “Accurate, Large Minibatch SGD: Training
ImageNet in 1 Hour” P. Goyal, P. Dollár, R. Girshick, P.
Noordhuis, L. Wesolowski, A. Kyrola, A.  Tulloch Y. Jia, and
K. He. 2018. <a class="reference external" href="https://arXiv.org/abs/1706.02677">https://arXiv.org/abs/1706.02677</a></p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
      <a href="https://github.com/stsievert/adadamp">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_gray_6d6d6d.png?resize=149%2C149" alt="Fork me on GitHub">
    </a>

          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="api.html" class="btn btn-neutral float-right" title="API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="math.html" class="btn btn-neutral float-left" title="Mathematical underpinnings" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Scott Sievert.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>